{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory is: C:\\Users\\is95217\\source\\turkishplate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import skimage\n",
    "import glob\n",
    "import datetime\n",
    "# Import Mask RCNN\n",
    "working_dir = os.getcwd()\n",
    "print(\"Working directory is:\",working_dir)\n",
    "sys.path.append(working_dir)  # To find local version of the library\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn.model import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DATA_DIR set to: /Users/burke.atilla\\Source/data/\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'custom' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a82769c94f08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ROOT_DATA_DIR set to:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mROOT_DATA_DIR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcustom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCustomConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mcustom_DIR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mROOT_DATA_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"originals\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"custom_DIR set to:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcustom_DIR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'custom' is not defined"
     ]
    }
   ],
   "source": [
    "#import custom \n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "custom_WEIGHTS_PATH = \"mask_rcnn_plate_0010.h5\"  # TODO: update this path\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#GPU MACHINE\n",
    "ROOT_DIR= \"/home/user/\"\n",
    "#LOCAL\n",
    "ROOT_DIR= \"/Users/burke.atilla\"\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DATA_DIR = os.path.join(ROOT_DIR ,'Source/data/')\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR= os.path.join(ROOT_DIR , 'Source/OPY_YapayZeka/Burke/mask_rcnn_plate_detection')\n",
    "print(\"ROOT_DATA_DIR set to:\",ROOT_DATA_DIR)\n",
    "\n",
    "config = custom.CustomConfig()\n",
    "custom_DIR = os.path.join(ROOT_DATA_DIR, \"originals\")\n",
    "print(\"custom_DIR set to:\",custom_DIR)\n",
    "\n",
    "print(\"MODEL_DIR set to:\",MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override the training configurations with a few\n",
    "# changes for inferencing.\n",
    "class InferenceConfig(config.__class__):\n",
    "    # Run detection on one image at a time\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device to load the neural network on.\n",
    "# Useful if you're training a model on the same \n",
    "# machine, in which case use CPU and leave the\n",
    "# GPU for training.\n",
    "DEVICE = \"/cpu:0\"  # /cpu:0 or /gpu:0\n",
    "#DEVICE = \"/cpu:0\"  # /cpu:0 or /gpu:0\n",
    "\n",
    "# Inspect the model in training or inference modes\n",
    "# values: 'inference' or 'training'\n",
    "# TODO: code for 'training' test mode not ready yet\n",
    "TEST_MODE = \"inference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=16):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Adjust the size attribute to control how big to render images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MODEL_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f378cc102680>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create model in inference mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mlocalize_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodellib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskRCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"inference\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMODEL_DIR\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# load the last model you trained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MODEL_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "# Create model in inference mode\n",
    "with tf.device(DEVICE):\n",
    "    localize_model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR,config=config)\n",
    "    \n",
    "# load the last model you trained\n",
    "# weights_path = model.find_last()[1]\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Load weights\n",
    "print(\"Loading weights \", custom_WEIGHTS_PATH)\n",
    "localize_model.load_weights(custom_WEIGHTS_PATH, by_name=True)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print (start_time - start_time)\n",
    "    \n",
    "    \n",
    "start_time = datetime.datetime.now()\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "\n",
    "image_path = '/home/user/Source/data/6000/1#34BT1889#16042018#093210.jpg'\n",
    "\n",
    "image_path = os.path.join(ROOT_DATA_DIR,'6000/1#34BT364#29032018#184143.jpg')\n",
    "\n",
    "image_path = os.path.join(ROOT_DATA_DIR,'6000/1#34BT1889#16042018#093210.jpg')\n",
    "\n",
    "image_path = os.path.join(ROOT_DATA_DIR,'12000/1#41AG237#04042018#103637.jpg') #sorun\n",
    "image_path = os.path.join(ROOT_DATA_DIR,'12000/1#41AJ145#10042018#173956.jpg') #sorun\n",
    "\n",
    "image_path = os.path.join(ROOT_DATA_DIR,'12000/1#41AL352#01072018#133612.jpg') #sorun\n",
    "\n",
    "image_path = os.path.join(ROOT_DATA_DIR,'12000/1#41AN968#09042018#130243.jpg') #sorun\n",
    "\n",
    "image_path = os.path.join(ROOT_DATA_DIR,'12000/1#41AR693#02042018#130027.jpg')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"image will be read:\",image_path)\n",
    "#image_path = '/home/user/Source/data/custom/'+'3.jpg'\n",
    "#image_path = '/home/user/Source/data/custom/'+'IMG_0667.JPG'\n",
    "\n",
    "image = skimage.io.imread(image_path)\n",
    "\n",
    "results = localize_model.detect([image], verbose=1)\n",
    "# Display results\n",
    "ax = get_ax(1)\n",
    "r = results[0]\n",
    "visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            ['Background','Plate'], r['scores'], ax=ax,\n",
    "                            title=\"Predictions\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"rois:\",r['rois'],\"scores:\",r['scores'])\n",
    "end_time = datetime.datetime.now()\n",
    "print (end_time - start_time)\n",
    "\n",
    "print(\"rois>\",r['rois'])\n",
    "x1 = r['rois'][0][0]\n",
    "y1 = r['rois'][0][1]\n",
    "x2 = r['rois'][0][2]\n",
    "y2 = r['rois'][0][3]\n",
    "\n",
    "plt.imshow(image[x1:x2,y1:y2])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLATE DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import os\n",
    "from keras import backend as K\n",
    "import itertools\n",
    "\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "\n",
    "#sys.path.append(os.path.dirname(os.path.realpath('Textgen')) + \"/python\")\n",
    "import Textgen as t\n",
    "\n",
    "#load model\n",
    "loaded_model=t.load_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters_train = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'Y', 'Z']\n",
    "letters = sorted(list(letters_train))\n",
    "\n",
    "def decode_batch(out):\n",
    "    ret = []\n",
    "    for j in range(out.shape[0]):\n",
    "        out_best = list(np.argmax(out[j, 2:], 1))\n",
    "        out_best = [k for k, g in itertools.groupby(out_best)]\n",
    "        outstr = ''\n",
    "        for c in out_best:\n",
    "            if c < len(letters):\n",
    "                outstr += letters[c]\n",
    "        ret.append(outstr)\n",
    "    return ret\n",
    "\n",
    "def predict_single_plate(sess,loaded_model,img):\n",
    "    img_w,img_h = 128, 64\n",
    "   \n",
    "    #img = cv2.imread(img_filepath)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = cv2.resize(img, (img_w, img_h))\n",
    "    img = img.astype(np.float32)\n",
    "    img /= 255\n",
    "\n",
    "\n",
    "    net_inp = loaded_model.get_layer(name='the_input').input\n",
    "    net_out = loaded_model.get_layer(name='softmax').output\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        X_data = np.ones([1, 1, img_w, img_h])\n",
    "    else:\n",
    "        X_data = np.ones([1, img_w, img_h, 1])\n",
    "    \n",
    "    img = img.T\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        img = np.expand_dims(img, 0)\n",
    "    else:\n",
    "        img = np.expand_dims(img, -1)\n",
    "    X_data[0] = img\n",
    "\n",
    "    net_out_value = sess.run(net_out, feed_dict={net_inp:X_data})\n",
    "    pred_texts = decode_batch(net_out_value)\n",
    "    print(pred_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict plate\n",
    "predicted_text = predict_single_plate(sess,loaded_model,image[x1:x2,y1:y2])#input directory of pic\n",
    "print(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
